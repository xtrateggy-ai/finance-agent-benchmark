#
# Stores environment (secret) variables for the Finance-Agent-Benchmark. 
#
# NOTE: For the LLM, the app uses: 
#       1) LLM_MODEL   
#       2) LLM_API_KEY
#        
# https://docs.litellm.ai/docs/providers
# https://docs.aimlapi.com/api-references/text-models-llm
# 
# HuggingFace Models: https://huggingface.co/models
#

# ---- LLM Models (litellm style) ----
LLM_MODEL=gemini/gemini-2.5-flash-lite
HF_LLM_MODEL=huggingface/Qwen/Qwen3-Next-80B-A3B-Instruct  # Hugging Face
OR_LLM_MODEL=openrouter/deepseek/deepseek-chat-v3.1:free   # Open Router

#LLM_MODEL=openai/gpt-4o
#LLM_MODEL=anthropic/claude-3-haiku
#LLM_MODEL=grok/grok-3
#LLM_MODEL=groq/llama3-8b-8192
#LLM_MODEL=together/mistral-7b-instruct
#LLM_MODEL=ollama/llama3.1
#LLM_MODEL=openrouter/meta-llama/llama-3-8b-instruct:free
#LLM_MODEL=anthropic/claude-3.5-sonnet
#LLM_MODEL=huggingface/MiniMaxAI/MiniMax-M2 # This works

# DEBUG Mode
DEBUG=true

#SEC_HEADERS
YOUR_NAME=TYPE YOUR NAME OR APP NAME
EMAIL_ADDRESS=TYPE YOUR EMAIL

# ---- LOCAL LLM & RAG
USE_LOCAL_LLM_WHITE=0    # 1=True 0=False - White agent tool decisions (NEW)
USE_LOCAL_LLM_JUDGE=0    # 1=True 0=False - Answer evaluation (NEW)
USE_LOCAL_LLM_RAG=1      # 1=True 0=False - Choose between local LLM+RAG and Regex() extraction
USE_LOCAL_LLM_GPU=1      # 1=True 0=False - Set use GPU for the local LLM, if available in the machine
MAX_FILINGS_PER_QUESTION=125  # Max number of filings to process per question.
LOCAL_LLM_MODEL_PATH=models/qwen2.5-3b-instruct-q4_k_m.gguf
#LOCAL_LLM_MODEL_PATH=models/llama-3.2-3b-instruct-q4_k_m.gguf  # NOTE: this is 3B (not 1B) parameers
LOCAL_LLM_JUDGE_MODEL=models/llama-3.2-1b-instruct-q4_k_m.gguf

LOCAL_LLM_WHITE_CONTEXT=6144   # White agent (needs more for tool descriptions)
LOCAL_LLM_JUDGE_CONTEXT=2048   # Judge (just comparing two strings)
LOCAL_LLM_RAG_CONTEXT=4096     # RAG extraction

# ---- Debug & Others ----
LITELLM_LOG=DEBUG
VERBOSE=1       # 1=True 0=False
SAFETY_CHECK=0  # 1=True 0=False - To perfomer or not safety check

# ---- API keys ----
LLM_API_KEY=<< your api-key here>>
HF_LLM_API_KEY=<< your api-key here>> # Hugging Face
OR_LLM_API_KEY=<< your api-key here>> # Open Router

SEC_API_KEY=your_key_here
SERP_API_KEY=your-serpapi-key
OPENAI_API_KEY=

# ---- googleapi.com ----
GOOGLE_SEARCH_API_KEY=your-real-api-key-here
GOOGLE_CX=your-cx-id-here

# ---- ports / hosts (local) ----
GREEN_AGENT_HOST=127.0.0.1
GREEN_AGENT_PORT=9000
GREEN_AGENT_NUM_TASKS=5  # Number tasks to process (Overriden by --num_tasks parameter)
GREEN_CARD=cards/green_card.toml

MCP_HOST=127.0.0.1
MCP_PORT=9001

WHITE_AGENT_HOST=127.0.0.1
WHITE_AGENT_PORT=8000
WHITE_AGENT_MAX_ITER=5  # Max ierations for each task in white_agent
WHITE_CARD=cards/white_card.toml

# ---- paths ----
SCHEMA_FILE=mcp_schema.json
#DATASET=data/Financial-QA-10k.csv
DATASET=data/public.csv
USE_DISK_CACHE=1  # 1=True 0=False - To save SEC Filings to disk

